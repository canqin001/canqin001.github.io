<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Can Qin</title> <meta name="author" content="Can Qin"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Salesforce-Logo.jpg?c5a7f5d2800a8c8007f6cec51535a321"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://canqin001.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">service</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Can</span> Qin </h1> <p class="desc"><a href="https://www.salesforceairesearch.com" rel="external nofollow noopener" target="_blank">Salesforce AI Research</a>, 181 Lytton Avenue, Palo Alto, CA, 94301, USA</p> </header> <article> <div class="profile float-left"> <figure> <picture> <img src="/assets/img/qincan01_5.JPG?7621c15d64fe7bcc79a57ccf1d6798c3" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="qincan01_5.JPG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> </div> </div> <div class="clearfix"> <p><strong>Email:</strong> <em>cqin[at]salesforce.com</em> or <em>qin.ca[at]northeastern.edu</em></p> <p>Hello and welcome! I’m currently embracing the exciting world of artificial intelligence as a Research Scientist at Salesforce AI Research. My journey is driven by a deep passion for <code class="language-plaintext highlighter-rouge">Generative AI</code> and <code class="language-plaintext highlighter-rouge">Multi-modal Learning</code>, with a focus on developing <code class="language-plaintext highlighter-rouge">Video/Image to Text (Understanding)</code> and <code class="language-plaintext highlighter-rouge">Text to Video/Image (Generation)</code> techniques.</p> <p>In 2023, I earned my Ph.D. from Northeastern University in Boston, USA. My research during this period was primarily centered around the realms of <code class="language-plaintext highlighter-rouge">Transfer Learning</code> and <code class="language-plaintext highlighter-rouge">Efficient AI</code>, where I delved into complex problems and innovative solutions.</p> <p>Before my Ph.D. journey, I obtained my B.E. degree from Xidian University in Xi’an, China, in 2018. This foundation laid the groundwork for my ongoing pursuit of knowledge and innovation.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">May, 2025</th> <td> CogAlign was accpcated by ACL findings and we have released BLIP-3o. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb, 2025</th> <td> We have two papers accpected by CVPR 25! Our latest paper CogAlign was released. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep, 2024</th> <td> Our Medical MLLM paper was accepcted by EMNLP 24 (Main)! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug, 2024</th> <td> The xGen-MM (BLIP3) and xGen-VideoSyn-1 were released to the public! We have a paper accepcted by TKDE and congrats to Yizhou! I have been invited as the reviewer of Nature Communications. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul, 2024</th> <td> We have one paper accepcted by ECCV 24! </td> </tr> <tr> <th scope="row" style="width: 20%">Feb, 2024</th> <td> We have one paper accepcted by CVPR 24! </td> </tr> <tr> <th scope="row" style="width: 20%">Nov, 2023</th> <td> Begin my journey at Salesforce Research in Palo Alto! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun, 2023</th> <td> I have passed the PhD Dissertation Defense and become Dr. Qin! </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/image-blip3o.jpg" class="preview z-depth-1 rounded" width="250px" height="auto" alt="image-blip3o.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chen2025blip3" class="col-sm-8"> <div class="title">BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset</div> <div class="author"> Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, <em>Can Qin</em>, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, and  others</div> <div class="periodical"> <em>arXiv preprint arXiv:2505.09568</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2505.09568" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/JiuhaiChen/BLIP3o" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/vidkv.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="vidkv.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tao2025plug" class="col-sm-8"> <div class="title">Plug-and-Play 1. x-Bit KV Cache Quantization for Video Large Language Models</div> <div class="author"> Keda Tao, Haoxuan You, Yang Sui, <em>Can Qin</em>, and Huan Wang</div> <div class="periodical"> <em>arXiv preprint arXiv:2503.16257</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2503.16257" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/KD-TAO/VidKV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/cogalign.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="cogalign.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="huang2025vision" class="col-sm-8"> <div class="title">Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding</div> <div class="author"> Kung-Hsiang Huang, <em>Can Qin</em>, Haoyi Qiu, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu</div> <div class="periodical"> <em>Annual Meeting of the Association for Computational Linguistics (Findings)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2502.11492" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/SalesforceAIResearch/CogAlign" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/dycoke-demo.gif" class="preview z-depth-1 rounded" width="250px" height="auto" alt="dycoke-demo.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tao2024dycoke" class="col-sm-8"> <div class="title">DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models</div> <div class="author"> Keda Tao, <em>Can Qin</em>, Haoxuan You, Yang Sui, and Huan Wang</div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2411.15024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/KD-TAO/DyCoke?tab=readme-ov-file" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/xgen-mm-vid1.gif" class="preview z-depth-1 rounded" width="250px" height="auto" alt="xgen-mm-vid1.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ryoo2024xgen" class="col-sm-8"> <div class="title">xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</div> <div class="author"> Michael S Ryoo, Honglu Zhou, Shrikant Kendre, <em>Can Qin</em>, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, and Juan Carlos Niebles</div> <div class="periodical"> <em>arXiv preprint arXiv:2410.16267</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2410.16267" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/xgen-videosyn-1.gif" class="preview z-depth-1 rounded" width="250px" height="auto" alt="xgen-videosyn-1.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qin2024xgen" class="col-sm-8"> <div class="title">xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations</div> <div class="author"> <em>Can Qin</em>, Congying Xia, Krithika Ramakrishnan, Michael Ryoo, Lifu Tu, Yihao Feng, Manli Shu, Honglu Zhou, Anas Awadalla, Jun Wang, and  others</div> <div class="periodical"> <em>arXiv preprint arXiv:2408.12590</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2408.12590" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/SalesforceAIResearch/xgen-videosyn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/blip3.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="blip3.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xue2024xgen" class="col-sm-8"> <div class="title">xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</div> <div class="author"> Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, and  others</div> <div class="periodical"> <em>arXiv preprint arXiv:2408.08872</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2408.08872" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/salesforce/LAVIS/tree/xgen-mm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/preference_data_st_llava_med.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="preference_data_st_llava_med.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sun2024st" class="col-sm-8"> <div class="title">Self-Training Large Language and Vision Assistant for Medical</div> <div class="author"> Guohao Sun, <em>Can Qin</em>, Huazhu Fu, Linwei Wang, and Zhiqiang Tao</div> <div class="periodical"> <em>In The 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2406.19973" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/heliossun/STLLaVA-Med" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/sq-llava.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="sq-llava.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sun2024sq" class="col-sm-8"> <div class="title">SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant</div> <div class="author"> Guohao Sun, <em>Can Qin</em>, Jiamian Wang, Zeyuan Chen, Ran Xu, and Zhiqiang Tao</div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2403.11299" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/heliossun/SQ-LLaVA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/hive.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="hive.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2023hive" class="col-sm-8"> <div class="title">HIVE: Harnessing Human Feedback for Instructional Visual Editing</div> <div class="author"> Shu Zhang*, Xinyi Yang*, Yihao Feng*, <em>Can Qin</em>, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu</div> <div class="periodical"> <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2303.09618" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/salesforce/HIVE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/unicontrol.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="unicontrol.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qin2023unicontrol" class="col-sm-8"> <div class="title">UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild</div> <div class="author"> <em>Can Qin</em>, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, and Ran Xu</div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2305.11147" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/salesforce/UniControl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://canqin001.github.io/UniControl-Page" class="btn btn-sm z-depth-0" role="button">Website</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/gluegen.png" class="preview z-depth-1 rounded" width="250px" height="auto" alt="gluegen.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qin2023gluegen" class="col-sm-8"> <div class="title">GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation</div> <div class="author"> <em>Can Qin</em>, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, and Ran Xu</div> <div class="periodical"> <em>International Conference on Computer Vision (ICCV)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/arXiv:2303.10056" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://github.com/salesforce/GlueGen" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://canqin001.github.io/GlueGen-Page/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%71%69%6E.%63%61@%6E%6F%72%74%68%65%61%73%74%65%72%6E.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=QCik-YcAAAAJ&amp;hl=en&amp;authuser=1" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/12282768" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/canqin001" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/can-qin-5b7b33168" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://dblp.org/pid/214/2488.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Can Qin. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>